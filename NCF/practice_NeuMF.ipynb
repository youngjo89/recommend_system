{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T07:33:16.438864Z",
     "start_time": "2020-08-10T07:33:16.434849Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "\n",
    "from neural_collaborative_filtering.Dataset import Dataset\n",
    "from neural_collaborative_filtering.evaluate import evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T06:45:53.042432Z",
     "start_time": "2020-08-10T06:45:53.034473Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4-tf'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 논문저자 코드 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T06:45:53.065372Z",
     "start_time": "2020-08-10T06:45:53.059411Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_instances(train, num_negatives):\n",
    "    user_input, item_input, labels = [],[],[]\n",
    "    num_users = train.shape[0]\n",
    "    num_items = train.shape[1]\n",
    "    for (u, i) in train.keys():\n",
    "        # positive instance\n",
    "        user_input.append(u)\n",
    "        item_input.append(i)\n",
    "        labels.append(1)\n",
    "        # negative instances\n",
    "        for t in range(num_negatives):\n",
    "            j = np.random.randint(num_items)\n",
    "            while (u, j) in train:\n",
    "                j = np.random.randint(num_items)\n",
    "            user_input.append(u)\n",
    "            item_input.append(j)\n",
    "            labels.append(0)\n",
    "    return user_input, item_input, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T06:46:03.232201Z",
     "start_time": "2020-08-10T06:45:53.088312Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(\"./neural_collaborative_filtering/Data/ml-1m\")\n",
    "train, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrain된 gmf, mlp 불러오고..합쳐서 neumf의 input으로 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T07:27:49.785431Z",
     "start_time": "2020-08-10T07:27:49.781441Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 256\n",
    "learning_rate = 0.001\n",
    "num_negatives = 4\n",
    "topK = 10\n",
    "evaluation_threads = 1\n",
    "\n",
    "# for gmf\n",
    "latent_dim = 8\n",
    "\n",
    "# for mlp\n",
    "embedding_size = 64\n",
    "\n",
    "# regs = [0, 0]\n",
    "\n",
    "# 모델 확인용.. data를 실제로 넣어서 학습할때는 없애야함.\n",
    "# user_num = 100\n",
    "# item_num = 100\n",
    "user_num = train.shape[0]\n",
    "item_num = train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T06:46:05.682343Z",
     "start_time": "2020-08-10T06:46:04.796995Z"
    }
   },
   "outputs": [],
   "source": [
    "# gmf\n",
    "# user, item을 동일한 size의 latent space로.. embedding..후 element wise product..\n",
    "user_input_gmf = keras.layers.Input(shape = (1,), dtype = \"int32\", name = \"user_input_gmf\")\n",
    "item_input_gmf = keras.layers.Input(shape = (1,), dtype = \"int32\", name = \"item_input_gmf\")\n",
    "\n",
    "\n",
    "user_embedding_gmf = keras.layers.Embedding(input_dim = user_num,\n",
    "                                            output_dim = latent_dim,\n",
    "                                            name = \"user_embedding_gmf\",\n",
    "#                                             embeddings_initializer = \"uniform\",\n",
    "                                            embeddings_initializer = keras.initializers.RandomNormal(mean = 0,\n",
    "                                                                                                     stddev = 0.01),\n",
    "#                                             embeddings_regularizer = 0,\n",
    "                                            input_length = 1)\n",
    "\n",
    "item_embedding_gmf = keras.layers.Embedding(input_dim = item_num,\n",
    "                                            output_dim = latent_dim,\n",
    "                                            name = \"item_embedding_gmf\",\n",
    "                                            embeddings_initializer = keras.initializers.RandomNormal(mean = 0,\n",
    "                                                                                                     stddev = 0.01),\n",
    "#                                             embeddings_regularizer = 0,\n",
    "                                            input_length = 1)\n",
    "\n",
    "user_latent_gmf = keras.layers.Flatten()(user_embedding_gmf(user_input_gmf))\n",
    "item_latent_gmf = keras.layers.Flatten()(item_embedding_gmf(item_input_gmf))\n",
    "\n",
    "element_wise_product = keras.layers.multiply([user_latent_gmf, item_latent_gmf])\n",
    "\n",
    "prediction_gmf = keras.layers.Dense(1,\n",
    "                                    activation = \"sigmoid\",\n",
    "                                    kernel_initializer = \"lecun_uniform\",\n",
    "                                    name = \"prediction_gmf\")(element_wise_product)\n",
    "\n",
    "model_gmf = keras.models.Model(inputs = [user_input_gmf, item_input_gmf],\n",
    "                               outputs = prediction_gmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T06:46:07.356014Z",
     "start_time": "2020-08-10T06:46:07.352038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_input_gmf (InputLayer)     [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_input_gmf (InputLayer)     [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_embedding_gmf (Embedding)  (None, 1, 8)         48320       user_input_gmf[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "item_embedding_gmf (Embedding)  (None, 1, 8)         29648       item_input_gmf[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8)            0           user_embedding_gmf[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8)            0           item_embedding_gmf[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 8)            0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "prediction_gmf (Dense)          (None, 1)            9           multiply[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 77,977\n",
      "Trainable params: 77,977\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_gmf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T06:46:06.565231Z",
     "start_time": "2020-08-10T06:46:06.464117Z"
    }
   },
   "outputs": [],
   "source": [
    "# mlp\n",
    "# user, item을 임베딩한후..concat, mlp layer 통과후 prediction 추출\n",
    "\n",
    "user_input_mlp = keras.layers.Input(shape = (1,), dtype = \"int32\", name = \"user_input_mlp\")\n",
    "item_input_mlp = keras.layers.Input(shape = (1,), dtype = \"int32\", name = \"item_input_mlp\")\n",
    "\n",
    "user_embedding_mlp = keras.layers.Embedding(input_dim = user_num,\n",
    "                                            output_dim = int(embedding_size / 2),\n",
    "                                            name = \"user_embedding_mlp\",\n",
    "                                            embeddings_initializer = keras.initializers.RandomNormal(mean = 0,\n",
    "                                                                                                     stddev = 0.01),\n",
    "#                                             embeddings_regularizer = 0,\n",
    "                                            input_length = 1)\n",
    "\n",
    "item_embedding_mlp = keras.layers.Embedding(input_dim = item_num,\n",
    "                                            output_dim = int(embedding_size / 2),\n",
    "                                            name = \"item_embedding_mlp\",\n",
    "                                            embeddings_initializer = keras.initializers.RandomNormal(mean = 0,\n",
    "                                                                                                     stddev = 0.01),\n",
    "#                                             embeddings_regularizer = 0,\n",
    "                                            input_length = 1)\n",
    "\n",
    "user_latent_mlp = keras.layers.Flatten()(user_embedding_mlp(user_input_mlp)) # 32\n",
    "item_latent_mlp = keras.layers.Flatten()(item_embedding_mlp(item_input_mlp)) # 32\n",
    "\n",
    "concat = keras.layers.concatenate([user_latent_mlp, item_latent_mlp]) # 64\n",
    "\n",
    "mlp_1 = keras.layers.Dense(units = embedding_size / 2, activation = \"relu\", name = \"mlp_1\")(concat)\n",
    "mlp_2 = keras.layers.Dense(units = embedding_size / 4, activation = \"relu\", name = \"mlp_2\")(mlp_1)\n",
    "mlp_3 = keras.layers.Dense(units = embedding_size / 8, activation = \"relu\", name = \"mlp_3\")(mlp_2)\n",
    "\n",
    "prediction_mlp = keras.layers.Dense(1,\n",
    "                                    activation = \"sigmoid\",\n",
    "                                    kernel_initializer = \"lecun_uniform\",\n",
    "                                    name = \"prediction_mlp\")(mlp_3)\n",
    "\n",
    "model_mlp = keras.models.Model(inputs = [user_input_mlp, item_input_mlp],\n",
    "                               outputs = prediction_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T06:46:08.165824Z",
     "start_time": "2020-08-10T06:46:08.161833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_input_mlp (InputLayer)     [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_input_mlp (InputLayer)     [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_embedding_mlp (Embedding)  (None, 1, 32)        193280      user_input_mlp[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "item_embedding_mlp (Embedding)  (None, 1, 32)        118592      item_input_mlp[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 32)           0           user_embedding_mlp[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 32)           0           item_embedding_mlp[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 64)           0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mlp_1 (Dense)                   (None, 32)           2080        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "mlp_2 (Dense)                   (None, 16)           528         mlp_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "mlp_3 (Dense)                   (None, 8)            136         mlp_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "prediction_mlp (Dense)          (None, 1)            9           mlp_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 314,625\n",
      "Trainable params: 314,625\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T06:04:04.296848Z",
     "start_time": "2020-08-10T06:04:04.268897Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NeuMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T06:49:28.755085Z",
     "start_time": "2020-08-10T06:49:28.665297Z"
    }
   },
   "outputs": [],
   "source": [
    "user_input_neumf = keras.layers.Input(shape = (1,), dtype = \"int32\", name = \"user_input_neumf\")\n",
    "item_input_neumf = keras.layers.Input(shape = (1,), dtype = \"int32\", name = \"item_input_neumf\")\n",
    "\n",
    "# gmf part\n",
    "user_embedding_gmf_neumf = keras.layers.Embedding(input_dim = user_num,\n",
    "                                                  output_dim = latent_dim,\n",
    "                                                  name = \"user_embedding_gmf_neumf\",\n",
    "                                                  embeddings_initializer = keras.initializers.RandomNormal(mean = 0,\n",
    "                                                                                                           stddev = 0.01),\n",
    "#                                                   embeddings_regularizer = 0,\n",
    "                                                  input_length = 1)\n",
    "\n",
    "item_embedding_gmf_neumf = keras.layers.Embedding(input_dim = item_num,\n",
    "                                                  output_dim = latent_dim,\n",
    "                                                  name = \"item_embedding_gmf_neumf\",\n",
    "                                                  embeddings_initializer = keras.initializers.RandomNormal(mean = 0,\n",
    "                                                                                                           stddev = 0.01),\n",
    "#                                                   embeddings_regularizer = 0,\n",
    "                                                  input_length = 1)\n",
    "\n",
    "user_latent_gmf_neumf = keras.layers.Flatten()(user_embedding_gmf_neumf(user_input_neumf))\n",
    "item_latent_gmf_neumf = keras.layers.Flatten()(item_embedding_gmf_neumf(item_input_neumf))\n",
    "\n",
    "element_wise_product_neumf = keras.layers.multiply([user_latent_gmf_neumf, item_latent_gmf_neumf])\n",
    "\n",
    "########################################################################################\n",
    "# mlp part\n",
    "user_embedding_mlp_neumf = keras.layers.Embedding(input_dim = user_num,\n",
    "                                            output_dim = int(embedding_size / 2),\n",
    "                                            name = \"user_embedding_mlp_neumf\",\n",
    "                                            embeddings_initializer = keras.initializers.RandomNormal(mean = 0,\n",
    "                                                                                                     stddev = 0.01),\n",
    "#                                             embeddings_regularizer = 0,\n",
    "                                            input_length = 1)\n",
    "\n",
    "item_embedding_mlp_neumf = keras.layers.Embedding(input_dim = item_num,\n",
    "                                            output_dim = int(embedding_size / 2),\n",
    "                                            name = \"item_embedding_mlp_neumf\",\n",
    "                                            embeddings_initializer = keras.initializers.RandomNormal(mean = 0,\n",
    "                                                                                                     stddev = 0.01),\n",
    "#                                             embeddings_regularizer = 0,\n",
    "                                            input_length = 1)\n",
    "\n",
    "user_latent_mlp_neumf = keras.layers.Flatten()(user_embedding_mlp_neumf(user_input_neumf)) # 32\n",
    "item_latent_mlp_neumf = keras.layers.Flatten()(item_embedding_mlp_neumf(item_input_neumf)) # 32\n",
    "\n",
    "concat_neumf = keras.layers.concatenate([user_latent_mlp_neumf, item_latent_mlp_neumf]) # 64\n",
    "\n",
    "mlp_1_neumf = keras.layers.Dense(units = embedding_size / 2, activation = \"relu\", name = \"mlp_1_neumf\")(concat_neumf)\n",
    "mlp_2_neumf = keras.layers.Dense(units = embedding_size / 4, activation = \"relu\", name = \"mlp_2_neumf\")(mlp_1_neumf)\n",
    "mlp_3_neumf = keras.layers.Dense(units = embedding_size / 8, activation = \"relu\", name = \"mlp_3_neumf\")(mlp_2_neumf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_neumf = keras.layers.concatenate([element_wise_product_neumf, mlp_3_neumf])\n",
    "\n",
    "prediction_neumf = keras.layers.Dense(1,\n",
    "                                      activation = \"sigmoid\",\n",
    "                                      kernel_initializer = \"lecun_uniform\",\n",
    "                                      name = \"prediction_neumf\")(input_neumf)\n",
    "\n",
    "model_neumf = keras.models.Model(inputs = [user_input_neumf, item_input_neumf],\n",
    "                                 outputs = prediction_neumf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T06:49:30.133672Z",
     "start_time": "2020-08-10T06:49:30.127688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_input_neumf (InputLayer)   [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_input_neumf (InputLayer)   [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_embedding_mlp_neumf (Embed (None, 1, 32)        193280      user_input_neumf[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "item_embedding_mlp_neumf (Embed (None, 1, 32)        118592      item_input_neumf[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 32)           0           user_embedding_mlp_neumf[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 32)           0           item_embedding_mlp_neumf[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64)           0           flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "user_embedding_gmf_neumf (Embed (None, 1, 8)         48320       user_input_neumf[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "item_embedding_gmf_neumf (Embed (None, 1, 8)         29648       item_input_neumf[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "mlp_1_neumf (Dense)             (None, 32)           2080        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 8)            0           user_embedding_gmf_neumf[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 8)            0           item_embedding_gmf_neumf[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "mlp_2_neumf (Dense)             (None, 16)           528         mlp_1_neumf[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 8)            0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mlp_3_neumf (Dense)             (None, 8)            136         mlp_2_neumf[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 16)           0           multiply_1[0][0]                 \n",
      "                                                                 mlp_3_neumf[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "prediction_neumf (Dense)        (None, 1)            17          concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 392,601\n",
      "Trainable params: 392,601\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_neumf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이렇게 만드는거 매우 귀찮음.. class로 코드짜는게 괜히 짜는게 아니다.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T06:11:55.009470Z",
     "start_time": "2020-08-10T06:11:54.991523Z"
    }
   },
   "outputs": [],
   "source": [
    "# load pretrained weights\n",
    "model_gmf.load_weights(\"./model/model_gmf.h5\")\n",
    "model_mlp.load_weights(\"./model/model_mlp.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T06:50:24.764298Z",
     "start_time": "2020-08-10T06:50:24.740361Z"
    }
   },
   "outputs": [],
   "source": [
    "weight_1 = model_gmf.get_layer(name = \"user_embedding_gmf\").get_weights()\n",
    "weight_2 = model_gmf.get_layer(name = \"item_embedding_gmf\").get_weights()\n",
    "weight_3 = model_gmf.get_layer(name = \"prediction_gmf\").get_weights()\n",
    "\n",
    "weight_4 = model_mlp.get_layer(name = \"user_embedding_mlp\").get_weights()\n",
    "weight_5 = model_mlp.get_layer(name = \"item_embedding_mlp\").get_weights()\n",
    "weight_6 = model_mlp.get_layer(name = \"mlp_1\").get_weights()\n",
    "weight_7 = model_mlp.get_layer(name = \"mlp_2\").get_weights()\n",
    "weight_8 = model_mlp.get_layer(name = \"mlp_3\").get_weights()\n",
    "weight_9 = model_mlp.get_layer(name = \"prediction_mlp\").get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T07:05:18.786687Z",
     "start_time": "2020-08-10T07:05:18.782678Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_weight_w = np.concatenate((weight_3[0], weight_9[0]), axis = 0)\n",
    "pred_weight_b = (weight_3[1] + weight_9[1]) / 2\n",
    "pred_weight = [pred_weight_w, pred_weight_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T07:05:26.946083Z",
     "start_time": "2020-08-10T07:05:26.924137Z"
    }
   },
   "outputs": [],
   "source": [
    "model_neumf.get_layer(name = \"user_embedding_gmf_neumf\").set_weights(weight_1)\n",
    "model_neumf.get_layer(name = \"item_embedding_gmf_neumf\").set_weights(weight_2)\n",
    "\n",
    "model_neumf.get_layer(name = \"user_embedding_mlp_neumf\").set_weights(weight_4)\n",
    "model_neumf.get_layer(name = \"item_embedding_mlp_neumf\").set_weights(weight_5)\n",
    "model_neumf.get_layer(name = \"mlp_1_neumf\").set_weights(weight_6)\n",
    "model_neumf.get_layer(name = \"mlp_2_neumf\").set_weights(weight_7)\n",
    "model_neumf.get_layer(name = \"mlp_3_neumf\").set_weights(weight_8)\n",
    "\n",
    "# prediction layer는 gmf, mlp trade off 존재하는데.. 0.5씩 곱해서 weight설정함..\n",
    "model_neumf.get_layer(name = \"prediction_neumf\").set_weights(pred_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T07:06:13.106081Z",
     "start_time": "2020-08-10T07:06:13.084161Z"
    }
   },
   "outputs": [],
   "source": [
    "model_neumf.compile(optimizer = keras.optimizers.Adam(learning_rate = learning_rate),\n",
    "                    loss = \"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T07:07:13.662501Z",
     "start_time": "2020-08-10T07:06:50.830488Z"
    }
   },
   "outputs": [],
   "source": [
    "user_input, item_input, labels = get_train_instances(train, num_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T07:26:58.799194Z",
     "start_time": "2020-08-10T07:07:18.599557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4970845 samples\n",
      "Epoch 1/20\n",
      "4970845/4970845 [==============================] - 58s 12us/sample - loss: 0.3177\n",
      "Epoch 2/20\n",
      "4970845/4970845 [==============================] - 57s 12us/sample - loss: 0.2732\n",
      "Epoch 3/20\n",
      "4970845/4970845 [==============================] - 60s 12us/sample - loss: 0.2617\n",
      "Epoch 4/20\n",
      "4970845/4970845 [==============================] - 59s 12us/sample - loss: 0.2553\n",
      "Epoch 5/20\n",
      "4970845/4970845 [==============================] - 60s 12us/sample - loss: 0.2505\n",
      "Epoch 6/20\n",
      "4970845/4970845 [==============================] - 60s 12us/sample - loss: 0.2464\n",
      "Epoch 7/20\n",
      "4970845/4970845 [==============================] - 60s 12us/sample - loss: 0.2429\n",
      "Epoch 8/20\n",
      "4970845/4970845 [==============================] - 60s 12us/sample - loss: 0.2398\n",
      "Epoch 9/20\n",
      "4970845/4970845 [==============================] - 59s 12us/sample - loss: 0.2369\n",
      "Epoch 10/20\n",
      "4970845/4970845 [==============================] - 59s 12us/sample - loss: 0.2343\n",
      "Epoch 11/20\n",
      "4970845/4970845 [==============================] - 58s 12us/sample - loss: 0.2319\n",
      "Epoch 12/20\n",
      "4970845/4970845 [==============================] - 59s 12us/sample - loss: 0.2298\n",
      "Epoch 13/20\n",
      "4970845/4970845 [==============================] - 59s 12us/sample - loss: 0.2279\n",
      "Epoch 14/20\n",
      "4970845/4970845 [==============================] - 59s 12us/sample - loss: 0.2261\n",
      "Epoch 15/20\n",
      "4970845/4970845 [==============================] - 58s 12us/sample - loss: 0.2244\n",
      "Epoch 16/20\n",
      "4970845/4970845 [==============================] - 58s 12us/sample - loss: 0.2229\n",
      "Epoch 17/20\n",
      "4970845/4970845 [==============================] - 59s 12us/sample - loss: 0.2215\n",
      "Epoch 18/20\n",
      "4970845/4970845 [==============================] - 60s 12us/sample - loss: 0.2203\n",
      "Epoch 19/20\n",
      "4970845/4970845 [==============================] - 60s 12us/sample - loss: 0.2191\n",
      "Epoch 20/20\n",
      "4970845/4970845 [==============================] - 59s 12us/sample - loss: 0.2180\n"
     ]
    }
   ],
   "source": [
    "hist = model_neumf.fit([np.array(user_input), np.array(item_input)],\n",
    "                       np.array(labels),\n",
    "                       batch_size = batch_size,\n",
    "                       epochs = epochs,\n",
    "                       verbose = 1,\n",
    "                       shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T07:35:19.370935Z",
     "start_time": "2020-08-10T07:33:29.771798Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  ...],\n",
       " [0.5,\n",
       "  0,\n",
       "  0.2890648263178878,\n",
       "  0.6309297535714574,\n",
       "  0.3562071871080222,\n",
       "  0.3868528072345416,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0.3868528072345416,\n",
       "  0.43067655807339306,\n",
       "  0.43067655807339306,\n",
       "  1.0,\n",
       "  0.5,\n",
       "  1.0,\n",
       "  0,\n",
       "  0,\n",
       "  0.5,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0.3868528072345416,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.30102999566398114,\n",
       "  0.30102999566398114,\n",
       "  0.30102999566398114,\n",
       "  0,\n",
       "  0.3868528072345416,\n",
       "  0,\n",
       "  0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0,\n",
       "  0.43067655807339306,\n",
       "  1.0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0.3562071871080222,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0.5,\n",
       "  0.3562071871080222,\n",
       "  0.30102999566398114,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.6309297535714574,\n",
       "  1.0,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  1.0,\n",
       "  0.33333333333333337,\n",
       "  0,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0.6309297535714574,\n",
       "  0.6309297535714574,\n",
       "  0.3154648767857287,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0.33333333333333337,\n",
       "  0.43067655807339306,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.5,\n",
       "  0,\n",
       "  0.3868528072345416,\n",
       "  1.0,\n",
       "  0,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0.5,\n",
       "  0,\n",
       "  1.0,\n",
       "  0,\n",
       "  0.30102999566398114,\n",
       "  0.30102999566398114,\n",
       "  0,\n",
       "  0,\n",
       "  0.5,\n",
       "  0.3562071871080222,\n",
       "  1.0,\n",
       "  0.5,\n",
       "  1.0,\n",
       "  0.6309297535714574,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.3868528072345416,\n",
       "  0.3562071871080222,\n",
       "  0.2890648263178878,\n",
       "  0.2890648263178878,\n",
       "  0,\n",
       "  0.3562071871080222,\n",
       "  0,\n",
       "  0.30102999566398114,\n",
       "  0.6309297535714574,\n",
       "  1.0,\n",
       "  0.3868528072345416,\n",
       "  1.0,\n",
       "  0.2890648263178878,\n",
       "  0,\n",
       "  0.3868528072345416,\n",
       "  0.3154648767857287,\n",
       "  0,\n",
       "  1.0,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0.3154648767857287,\n",
       "  0,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  1.0,\n",
       "  0,\n",
       "  0,\n",
       "  0.5,\n",
       "  0.6309297535714574,\n",
       "  1.0,\n",
       "  0.5,\n",
       "  0.30102999566398114,\n",
       "  1.0,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.33333333333333337,\n",
       "  0.33333333333333337,\n",
       "  0,\n",
       "  0,\n",
       "  0.2890648263178878,\n",
       "  0,\n",
       "  0,\n",
       "  0.3868528072345416,\n",
       "  0.6309297535714574,\n",
       "  0.3562071871080222,\n",
       "  1.0,\n",
       "  0,\n",
       "  0.43067655807339306,\n",
       "  0.2890648263178878,\n",
       "  0.6309297535714574,\n",
       "  1.0,\n",
       "  0,\n",
       "  0.33333333333333337,\n",
       "  0,\n",
       "  0.5,\n",
       "  0.3562071871080222,\n",
       "  0.5,\n",
       "  0.33333333333333337,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0.3868528072345416,\n",
       "  0.5,\n",
       "  1.0,\n",
       "  0.33333333333333337,\n",
       "  0.30102999566398114,\n",
       "  0,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  1.0,\n",
       "  0,\n",
       "  0.33333333333333337,\n",
       "  0.6309297535714574,\n",
       "  0.3868528072345416,\n",
       "  1.0,\n",
       "  0,\n",
       "  0.30102999566398114,\n",
       "  0,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.6309297535714574,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.3562071871080222,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0.3868528072345416,\n",
       "  0,\n",
       "  0,\n",
       "  0.33333333333333337,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.5,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0.3868528072345416,\n",
       "  0,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0,\n",
       "  0.30102999566398114,\n",
       "  1.0,\n",
       "  0.30102999566398114,\n",
       "  0.5,\n",
       "  0.3562071871080222,\n",
       "  0,\n",
       "  0.43067655807339306,\n",
       "  1.0,\n",
       "  0,\n",
       "  0.3154648767857287,\n",
       "  0.43067655807339306,\n",
       "  0,\n",
       "  0,\n",
       "  0.33333333333333337,\n",
       "  1.0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.6309297535714574,\n",
       "  1.0,\n",
       "  0.6309297535714574,\n",
       "  0.6309297535714574,\n",
       "  0.6309297535714574,\n",
       "  0.43067655807339306,\n",
       "  1.0,\n",
       "  0,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.33333333333333337,\n",
       "  0.43067655807339306,\n",
       "  0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.2890648263178878,\n",
       "  0.33333333333333337,\n",
       "  0,\n",
       "  0.3154648767857287,\n",
       "  1.0,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.5,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  1.0,\n",
       "  0,\n",
       "  0.33333333333333337,\n",
       "  0,\n",
       "  0.33333333333333337,\n",
       "  0,\n",
       "  1.0,\n",
       "  0,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.3562071871080222,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0,\n",
       "  0.5,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.3154648767857287,\n",
       "  0,\n",
       "  0.2890648263178878,\n",
       "  0.6309297535714574,\n",
       "  0.3562071871080222,\n",
       "  1.0,\n",
       "  0.6309297535714574,\n",
       "  1.0,\n",
       "  0.43067655807339306,\n",
       "  0,\n",
       "  0.33333333333333337,\n",
       "  0.3154648767857287,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.6309297535714574,\n",
       "  0.3868528072345416,\n",
       "  0.5,\n",
       "  0.3154648767857287,\n",
       "  0.6309297535714574,\n",
       "  0.43067655807339306,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0,\n",
       "  0.43067655807339306,\n",
       "  0,\n",
       "  0.5,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0.3562071871080222,\n",
       "  0.3562071871080222,\n",
       "  0,\n",
       "  0,\n",
       "  0.3562071871080222,\n",
       "  0.6309297535714574,\n",
       "  0.30102999566398114,\n",
       "  0.33333333333333337,\n",
       "  0.6309297535714574,\n",
       "  0.3154648767857287,\n",
       "  0,\n",
       "  0.2890648263178878,\n",
       "  0,\n",
       "  0,\n",
       "  0.43067655807339306,\n",
       "  0.3154648767857287,\n",
       "  0,\n",
       "  0.33333333333333337,\n",
       "  0.33333333333333337,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0.3562071871080222,\n",
       "  0,\n",
       "  0.33333333333333337,\n",
       "  0.43067655807339306,\n",
       "  0,\n",
       "  0.43067655807339306,\n",
       "  0.43067655807339306,\n",
       "  0.3868528072345416,\n",
       "  0.43067655807339306,\n",
       "  0,\n",
       "  0.3562071871080222,\n",
       "  1.0,\n",
       "  0.30102999566398114,\n",
       "  0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0,\n",
       "  0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.5,\n",
       "  0.3868528072345416,\n",
       "  0.5,\n",
       "  0.3868528072345416,\n",
       "  0,\n",
       "  0.43067655807339306,\n",
       "  0,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.43067655807339306,\n",
       "  1.0,\n",
       "  0.6309297535714574,\n",
       "  1.0,\n",
       "  0,\n",
       "  0.3154648767857287,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.5,\n",
       "  0.3868528072345416,\n",
       "  0,\n",
       "  0.3562071871080222,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0.6309297535714574,\n",
       "  0.33333333333333337,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0.3868528072345416,\n",
       "  0.5,\n",
       "  0.3562071871080222,\n",
       "  0,\n",
       "  0,\n",
       "  0.3868528072345416,\n",
       "  1.0,\n",
       "  0.3868528072345416,\n",
       "  0.3154648767857287,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.6309297535714574,\n",
       "  0.2890648263178878,\n",
       "  0,\n",
       "  1.0,\n",
       "  0,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.33333333333333337,\n",
       "  0,\n",
       "  0.3154648767857287,\n",
       "  0.33333333333333337,\n",
       "  0,\n",
       "  0,\n",
       "  0.33333333333333337,\n",
       "  0.2890648263178878,\n",
       "  0,\n",
       "  0,\n",
       "  0.5,\n",
       "  1.0,\n",
       "  0.43067655807339306,\n",
       "  1.0,\n",
       "  0.43067655807339306,\n",
       "  0.30102999566398114,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.2890648263178878,\n",
       "  0,\n",
       "  0,\n",
       "  0.3562071871080222,\n",
       "  0.3154648767857287,\n",
       "  0,\n",
       "  0.43067655807339306,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.43067655807339306,\n",
       "  0,\n",
       "  0,\n",
       "  0.43067655807339306,\n",
       "  0.3562071871080222,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0.3562071871080222,\n",
       "  0,\n",
       "  0.30102999566398114,\n",
       "  1.0,\n",
       "  0.2890648263178878,\n",
       "  1.0,\n",
       "  0,\n",
       "  0.3868528072345416,\n",
       "  0.6309297535714574,\n",
       "  0.3154648767857287,\n",
       "  0,\n",
       "  0.5,\n",
       "  0.3868528072345416,\n",
       "  0,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0.3154648767857287,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0.3868528072345416,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.33333333333333337,\n",
       "  0.6309297535714574,\n",
       "  0.5,\n",
       "  0.43067655807339306,\n",
       "  0,\n",
       "  0.3868528072345416,\n",
       "  0.5,\n",
       "  1.0,\n",
       "  0.3868528072345416,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0.43067655807339306,\n",
       "  0.5,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0.43067655807339306,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0.3868528072345416,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  1.0,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.3868528072345416,\n",
       "  0.3154648767857287,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.6309297535714574,\n",
       "  0.6309297535714574,\n",
       "  0.6309297535714574,\n",
       "  0.3562071871080222,\n",
       "  0,\n",
       "  0.33333333333333337,\n",
       "  0,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.33333333333333337,\n",
       "  0.6309297535714574,\n",
       "  0.3868528072345416,\n",
       "  0,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.3868528072345416,\n",
       "  0.3562071871080222,\n",
       "  0.6309297535714574,\n",
       "  0.3562071871080222,\n",
       "  0.33333333333333337,\n",
       "  0.33333333333333337,\n",
       "  1.0,\n",
       "  0.3868528072345416,\n",
       "  1.0,\n",
       "  0,\n",
       "  0,\n",
       "  0.2890648263178878,\n",
       "  0.3562071871080222,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.30102999566398114,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0.3868528072345416,\n",
       "  0.3868528072345416,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0.6309297535714574,\n",
       "  0.3868528072345416,\n",
       "  0.3868528072345416,\n",
       "  0.3868528072345416,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.5,\n",
       "  0.43067655807339306,\n",
       "  0,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0,\n",
       "  1.0,\n",
       "  0,\n",
       "  0.3868528072345416,\n",
       "  0,\n",
       "  0.3562071871080222,\n",
       "  0.33333333333333337,\n",
       "  0.3868528072345416,\n",
       "  0.5,\n",
       "  0,\n",
       "  0,\n",
       "  0.5,\n",
       "  0.3154648767857287,\n",
       "  0.43067655807339306,\n",
       "  0.3562071871080222,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0.43067655807339306,\n",
       "  0.43067655807339306,\n",
       "  0,\n",
       "  1.0,\n",
       "  0,\n",
       "  0.5,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.43067655807339306,\n",
       "  0.3868528072345416,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.3562071871080222,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.2890648263178878,\n",
       "  0.3154648767857287,\n",
       "  1.0,\n",
       "  0.43067655807339306,\n",
       "  0.3868528072345416,\n",
       "  0.5,\n",
       "  0.3562071871080222,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0.5,\n",
       "  0,\n",
       "  0.3868528072345416,\n",
       "  0.5,\n",
       "  0.3154648767857287,\n",
       "  0.43067655807339306,\n",
       "  0.43067655807339306,\n",
       "  0.6309297535714574,\n",
       "  1.0,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0.33333333333333337,\n",
       "  0.33333333333333337,\n",
       "  0.43067655807339306,\n",
       "  0,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.6309297535714574,\n",
       "  0.3868528072345416,\n",
       "  0.3562071871080222,\n",
       "  0,\n",
       "  1.0,\n",
       "  0,\n",
       "  0.30102999566398114,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0.5,\n",
       "  1.0,\n",
       "  0.3868528072345416,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0.3154648767857287,\n",
       "  0.2890648263178878,\n",
       "  0.3562071871080222,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0.43067655807339306,\n",
       "  0.3868528072345416,\n",
       "  1.0,\n",
       "  0.33333333333333337,\n",
       "  0.5,\n",
       "  0.3868528072345416,\n",
       "  0,\n",
       "  0,\n",
       "  1.0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0.43067655807339306,\n",
       "  1.0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0.5,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0.33333333333333337,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0.6309297535714574,\n",
       "  0.3154648767857287,\n",
       "  0,\n",
       "  0,\n",
       "  0.3868528072345416,\n",
       "  0.3868528072345416,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0.5,\n",
       "  0,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  1.0,\n",
       "  0.3154648767857287,\n",
       "  0,\n",
       "  0.3868528072345416,\n",
       "  0.6309297535714574,\n",
       "  0.5,\n",
       "  0.33333333333333337,\n",
       "  0.5,\n",
       "  0.6309297535714574,\n",
       "  0.43067655807339306,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.43067655807339306,\n",
       "  0.6309297535714574,\n",
       "  0.2890648263178878,\n",
       "  1.0,\n",
       "  0,\n",
       "  0,\n",
       "  0.43067655807339306,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0.30102999566398114,\n",
       "  1.0,\n",
       "  0.30102999566398114,\n",
       "  0.5,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0.43067655807339306,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0.43067655807339306,\n",
       "  1.0,\n",
       "  0.6309297535714574,\n",
       "  0.3562071871080222,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0,\n",
       "  0.43067655807339306,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0.3562071871080222,\n",
       "  1.0,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0.6309297535714574,\n",
       "  0.30102999566398114,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.6309297535714574,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.3562071871080222,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0,\n",
       "  0.5,\n",
       "  1.0,\n",
       "  0.5,\n",
       "  0,\n",
       "  0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0,\n",
       "  1.0,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.3154648767857287,\n",
       "  0.3868528072345416,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0.5,\n",
       "  0.43067655807339306,\n",
       "  0.3154648767857287,\n",
       "  0.6309297535714574,\n",
       "  0.30102999566398114,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.5,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.6309297535714574,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.3562071871080222,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0.43067655807339306,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0.5,\n",
       "  0.3562071871080222,\n",
       "  0.5,\n",
       "  0,\n",
       "  0.3154648767857287,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0.5,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.5,\n",
       "  0.30102999566398114,\n",
       "  0.3868528072345416,\n",
       "  0,\n",
       "  1.0,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.3868528072345416,\n",
       "  0,\n",
       "  0,\n",
       "  0.3154648767857287,\n",
       "  0,\n",
       "  1.0,\n",
       "  0,\n",
       "  0.30102999566398114,\n",
       "  0,\n",
       "  0.33333333333333337,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.30102999566398114,\n",
       "  0,\n",
       "  0.3154648767857287,\n",
       "  0.3868528072345416,\n",
       "  0.3562071871080222,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0.3562071871080222,\n",
       "  1.0,\n",
       "  0.6309297535714574,\n",
       "  0.6309297535714574,\n",
       "  0.2890648263178878,\n",
       "  0.33333333333333337,\n",
       "  0,\n",
       "  0.3868528072345416,\n",
       "  1.0,\n",
       "  0.6309297535714574,\n",
       "  0.33333333333333337,\n",
       "  0.3562071871080222,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.3562071871080222,\n",
       "  1.0,\n",
       "  0,\n",
       "  1.0,\n",
       "  0,\n",
       "  0.30102999566398114,\n",
       "  1.0,\n",
       "  0,\n",
       "  0.43067655807339306,\n",
       "  0.5,\n",
       "  0.3562071871080222,\n",
       "  0.3562071871080222,\n",
       "  0.5,\n",
       "  0,\n",
       "  0.33333333333333337,\n",
       "  1.0,\n",
       "  0,\n",
       "  0,\n",
       "  0.2890648263178878,\n",
       "  0.43067655807339306,\n",
       "  0,\n",
       "  0,\n",
       "  0.43067655807339306,\n",
       "  0.6309297535714574,\n",
       "  0.3868528072345416,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.3868528072345416,\n",
       "  1.0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.5,\n",
       "  0.30102999566398114,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0,\n",
       "  0.3562071871080222,\n",
       "  0.3154648767857287,\n",
       "  1.0,\n",
       "  0.43067655807339306,\n",
       "  0.5,\n",
       "  0,\n",
       "  1.0,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  0.43067655807339306,\n",
       "  0,\n",
       "  0.3868528072345416,\n",
       "  0.2890648263178878,\n",
       "  0.6309297535714574,\n",
       "  0.43067655807339306,\n",
       "  0.43067655807339306,\n",
       "  0,\n",
       "  0,\n",
       "  0.33333333333333337,\n",
       "  0,\n",
       "  0.2890648263178878,\n",
       "  0.3562071871080222,\n",
       "  0,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.2890648263178878,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.3868528072345416,\n",
       "  0.30102999566398114,\n",
       "  0.43067655807339306,\n",
       "  0.3562071871080222,\n",
       "  0,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.6309297535714574,\n",
       "  0.33333333333333337,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.5,\n",
       "  0,\n",
       "  0,\n",
       "  0.3562071871080222,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0.43067655807339306,\n",
       "  1.0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0.43067655807339306,\n",
       "  0.30102999566398114,\n",
       "  1.0,\n",
       "  0.33333333333333337,\n",
       "  0,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0.30102999566398114,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0.6309297535714574,\n",
       "  0.33333333333333337,\n",
       "  0.5,\n",
       "  0.3562071871080222,\n",
       "  1.0,\n",
       "  0.6309297535714574,\n",
       "  0.43067655807339306,\n",
       "  0.33333333333333337,\n",
       "  0,\n",
       "  1.0,\n",
       "  0,\n",
       "  1.0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0.43067655807339306,\n",
       "  1.0,\n",
       "  0.6309297535714574,\n",
       "  0,\n",
       "  1.0,\n",
       "  0.43067655807339306,\n",
       "  0,\n",
       "  0.5,\n",
       "  0.3562071871080222,\n",
       "  0.3562071871080222,\n",
       "  0,\n",
       "  0,\n",
       "  0.5,\n",
       "  0.6309297535714574,\n",
       "  0.30102999566398114,\n",
       "  0.5,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  ...])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(hits, ndcgs) = evaluate_model(model_neumf, testRatings, testNegatives, topK, evaluation_threads)\n",
    "hits, ndcgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T07:36:16.752094Z",
     "start_time": "2020-08-10T07:36:16.747087Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.633774834437086, 0.37051792461740063)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(hits).mean(), np.array(ndcgs).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class로 구현하는건 구현자체는 조금 귀찮을지몰라도 나중에 써먹기엔 진짜좋다..\n",
    "# model.sequental() , add() 이런걸 왜 사용하지않았을까..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
